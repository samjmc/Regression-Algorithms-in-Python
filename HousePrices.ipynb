{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing the required libraries\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "\n",
    "## Drop rows with missing target values\n",
    "train_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "\n",
    "## Define target (y) and features (X)\n",
    "y = train_data['SalePrice']\n",
    "X = train_data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])\n",
    "\n",
    "## Now, ensure y is aligned with X after dropping missing values\n",
    "y = y[X.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the data into training and testing sets\n",
    "## 25% of the data will be used for testing, the rest for training\n",
    "## Feature (x) and target (y) splits \n",
    "train_X, test_X, train_y, test_y = train_test_split(X.values, y.values, test_size=0.25, random_state=42)\n",
    "\n",
    "## Initialise the imputer (you can choose strategy: 'mean', 'median', or 'most_frequent')\n",
    "my_imputer = SimpleImputer()  # Default is 'mean', but can choose others\n",
    "\n",
    "## Fit and transform the training data (impute missing values in the training set)\n",
    "train_X = my_imputer.fit_transform(train_X)\n",
    "\n",
    "## Transform the test data using the same imputer (to avoid data leakage)\n",
    "test_X = my_imputer.transform(test_X)\n",
    "\n",
    "## Optionally, you can convert back to DataFrame with the original column names after imputation\n",
    "train_X = pd.DataFrame(train_X, columns=X.columns)\n",
    "test_X = pd.DataFrame(test_X, columns=X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error using Decision Tress : 24616.879452054796\n"
     ]
    }
   ],
   "source": [
    "## Predictions using the Decision Tree algorithm \n",
    "\n",
    "decision_model = DecisionTreeRegressor()  \n",
    "decision_model.fit(train_X, train_y) \n",
    "predicted_decision_trees = decision_model.predict(test_X)\n",
    "print (\"Mean Absolute Error using Decision Tress :\", mean_absolute_error(test_y, predicted_decision_trees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error using Random Forest: 17865.90279532655\n"
     ]
    }
   ],
   "source": [
    "## Predictions using the Random Forest algorithm \n",
    "\n",
    "forest_model = RandomForestRegressor(n_estimators=100, max_depth=10)\n",
    "forest_model.fit(train_X, train_y )\n",
    "predicted_random_forest = forest_model.predict(test_X)\n",
    "print(\"Mean Absolute Error using Random Forest:\", mean_absolute_error(test_y, predicted_random_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error using XGBoost:  18624.489811643834\n"
     ]
    }
   ],
   "source": [
    "## Predictions using the XGBoost algorithm \n",
    "\n",
    "xg_model = XGBRegressor(n_estimators=100)\n",
    "xg_model.fit(train_X, train_y)\n",
    "predicted_XGBoost = xg_model.predict(test_X)\n",
    "print(\"Mean Absolute Error using XGBoost: \", mean_absolute_error(test_y, predicted_XGBoost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 216 candidates, totalling 432 fits\n",
      "Best parameters: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best score: 0.8351373963090232\n",
      "Mean Absolute Error using optimised Random Forest: 16671.756614480837\n"
     ]
    }
   ],
   "source": [
    "## Let's improve the RandomForest Model as it was the best performing in MAE\n",
    "## We will use GridSearchCV to tune the hyperparameters of the RandomForest Model\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "## Define the hyperparameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'] \n",
    "}\n",
    "\n",
    "\n",
    "## Create a RandomForestRegressor model\n",
    "rf_model = RandomForestRegressor()\n",
    "\n",
    "## Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=2, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(train_X, train_y)\n",
    "\n",
    "## Get the best parameters and best score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "\n",
    "## Predict with the best model\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "predicted_random_forest = best_rf_model.predict(test_X)\n",
    "print(\"Mean Absolute Error using optimised Random Forest:\", mean_absolute_error(test_y, predicted_random_forest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Features by Importance:\n",
      "OverallQual: 0.5608175571193543\n",
      "GrLivArea: 0.12505674606008982\n",
      "TotalBsmtSF: 0.042398733456264874\n",
      "2ndFlrSF: 0.03097132579538323\n",
      "BsmtFinSF1: 0.030612937013561166\n",
      "1stFlrSF: 0.029745973569876995\n",
      "LotArea: 0.01850612389103041\n",
      "GarageCars: 0.018335077484224884\n",
      "YearBuilt: 0.01751988296224242\n",
      "GarageArea: 0.01731046911747853\n"
     ]
    }
   ],
   "source": [
    "## Get the feature importance scores\n",
    "feature_importances = forest_model.feature_importances_\n",
    "\n",
    "## Create a list of features with their corresponding importance scores\n",
    "feature_importance_dict = {\n",
    "    feature: importance for feature, importance in zip(X.columns, feature_importances)\n",
    "}\n",
    "\n",
    "## Sort the features by their importance score (in descending order)\n",
    "sorted_features = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "## Display the top 10 most important features\n",
    "top_10_features = sorted_features[:10]\n",
    "print(\"Top 10 Features by Importance:\")\n",
    "for feature, importance in top_10_features:\n",
    "    print(f\"{feature}: {importance}\")\n",
    "\n",
    "## Random Forest inherently handles feature importance so it isn't necessary to include in tuning process. Still interesting to see..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HousePrices",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
